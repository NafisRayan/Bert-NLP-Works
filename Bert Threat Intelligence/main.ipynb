{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQmK30Il2ERN",
        "outputId": "315eaea3-df54-40d0-f640-e15da13ad8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/1\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4567 Acc: 0.8333\n",
            "Val Loss: 0.3689 Acc: 0.8947\n",
            "New best model saved with accuracy 0.8947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [01:21<00:00, 13.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Metrics:\n",
            "Accuracy: 0.7708\n",
            "Precision: 0.7708\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.8706\n",
            "\n",
            "Threat Analysis Results:\n",
            "\n",
            "Text: A new variant of Ryuk ransomware targeting healthcare systems was detected\n",
            "Prediction: MALICIOUS (Confidence: 0.91)\n",
            "Ground Truth: MALICIOUS\n",
            "Detected Entities: {'ransomware', 'sector'}\n",
            "\n",
            "Text: Normal system update process completed successfully\n",
            "Prediction: MALICIOUS (Confidence: 0.90)\n",
            "Ground Truth: BENIGN\n",
            "Detected Entities: {'software'}\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# pip install transformers torch datasets pandas numpy tqdm scikit-learn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    MODEL_NAME = 'bert-base-uncased'\n",
        "    MAX_LEN = 256  # Increased for threat intelligence text length\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 1\n",
        "    LEARNING_RATE = 2e-5\n",
        "    OUTPUT_DIR = './bert_threat_model/'\n",
        "    SEED = 42\n",
        "    MALICIOUS_LABELS = {  # Define malicious entity labels\n",
        "        'attack-pattern', 'malware', 'threat-actor',\n",
        "        'Infrastructure', 'C&C', 'exploit', 'ransomware'\n",
        "    }\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(config.SEED)\n",
        "np.random.seed(config.SEED)\n",
        "\n",
        "# Load and preprocess TSV dataset\n",
        "def load_threat_data(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Extract text\n",
        "        texts.append(row['text'])\n",
        "\n",
        "        # Process entities to determine label\n",
        "        try:\n",
        "            entities = ast.literal_eval(row['entities'])\n",
        "            entity_labels = {e['label'] for e in entities}\n",
        "            label = 1 if len(entity_labels & config.MALICIOUS_LABELS) > 0 else 0\n",
        "        except:\n",
        "            label = 0  # Default to benign if parsing fails\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Load dataset\n",
        "texts, labels = load_threat_data('Cyber-Threat-Intelligence-Custom-Data.tsv')  # Replace with your TSV path\n",
        "\n",
        "# Split dataset\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=config.SEED\n",
        ")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=config.SEED\n",
        ")\n",
        "\n",
        "# Dataset class\n",
        "class ThreatDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Create data loaders\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size, shuffle=False):\n",
        "    dataset = ThreatDataset(texts, labels, tokenizer, max_len)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = create_data_loader(train_texts, train_labels, tokenizer, config.MAX_LEN, config.BATCH_SIZE, shuffle=True)\n",
        "val_loader = create_data_loader(val_texts, val_labels, tokenizer, config.MAX_LEN, config.BATCH_SIZE)\n",
        "test_loader = create_data_loader(test_texts, test_labels, tokenizer, config.MAX_LEN, config.BATCH_SIZE)\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, correct_bias=False)\n",
        "total_steps = len(train_loader) * config.EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training loop with progress tracking\n",
        "best_accuracy = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{config.EPOCHS}')\n",
        "    print('-' * 50)\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_predictions.double() / len(train_loader.dataset)\n",
        "    history['train_loss'].append(epoch_loss)\n",
        "    history['train_acc'].append(epoch_acc)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validation', leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            val_correct_predictions += torch.sum(preds == labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_epoch_loss = val_loss / len(val_loader)\n",
        "    val_epoch_acc = val_correct_predictions.double() / len(val_loader.dataset)\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_epoch_acc > best_accuracy:\n",
        "        best_accuracy = val_epoch_acc\n",
        "        if not os.path.exists(config.OUTPUT_DIR):\n",
        "            os.makedirs(config.OUTPUT_DIR)\n",
        "        model.save_pretrained(config.OUTPUT_DIR)\n",
        "        tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "        print(f'New best model saved with accuracy {best_accuracy:.4f}')\n",
        "\n",
        "# Final evaluation on test set\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predictions, average='binary'\n",
        "    )\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "test_metrics = evaluate_model(model, test_loader)\n",
        "print('\\nTest Set Metrics:')\n",
        "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "\n",
        "# Inference function using saved model\n",
        "class ThreatAnalyzer:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.max_len = config.MAX_LEN\n",
        "        self.malicious_labels = config.MALICIOUS_LABELS\n",
        "\n",
        "    def analyze(self, text, entities):\n",
        "        # Preprocess entities\n",
        "        try:\n",
        "            parsed_entities = ast.literal_eval(entities)\n",
        "            entity_labels = {e['label'] for e in parsed_entities}\n",
        "            ground_truth = 1 if len(entity_labels & self.malicious_labels) > 0 else 0\n",
        "        except:\n",
        "            ground_truth = 0\n",
        "\n",
        "        # Model prediction\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "            _, prediction = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'prediction': 'malicious' if prediction.item() == 1 else 'benign',\n",
        "            'confidence': torch.softmax(outputs.logits, dim=1)[0].max().item(),\n",
        "            'ground_truth': 'malicious' if ground_truth == 1 else 'benign',\n",
        "            'entities': entity_labels\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    # Initialize analyzer with saved model\n",
        "    analyzer = ThreatAnalyzer(config.OUTPUT_DIR)\n",
        "\n",
        "    # Test with sample data\n",
        "    test_data = [\n",
        "        {\n",
        "            'text': \"A new variant of Ryuk ransomware targeting healthcare systems was detected\",\n",
        "            'entities': \"[{'label': 'ransomware'}, {'label': 'sector'}]\"\n",
        "        },\n",
        "        {\n",
        "            'text': \"Normal system update process completed successfully\",\n",
        "            'entities': \"[{'label': 'software'}]\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nThreat Analysis Results:\")\n",
        "    for example in test_data:\n",
        "        result = analyzer.analyze(example['text'], example['entities'])\n",
        "        print(f\"\\nText: {result['text']}\")\n",
        "        print(f\"Prediction: {result['prediction'].upper()} (Confidence: {result['confidence']:.2f})\")\n",
        "        print(f\"Ground Truth: {result['ground_truth'].upper()}\")\n",
        "        print(f\"Detected Entities: {result['entities']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeG2xrJ_2Swp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}